{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from collections import Counter\n",
    "from keras_visualizer import visualizer \n",
    "from keras.utils.vis_utils import plot_model\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import json\n",
    "import os\n",
    "import import_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\\\n",
    "    \"LABEL\",\"A_P_L1\",\"A_V_L1\",\"B_P_L1\",\n",
    "    \"B_V_L1\",\"A_P_L2\",\"A_V_L2\",\"B_P_L2\",\n",
    "    \"B_V_L2\",\"A_P_L3\",\"A_V_L3\",\"B_P_L3\",\n",
    "    \"B_V_L3\",\"A_P_L4\",\"A_V_L4\",\"B_P_L4\",\n",
    "    \"B_V_L4\",\"LC_1\",\"LC_2\",\"LC_3\",\"LC_4\",\n",
    "    \"LC_5\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_input_function(frame,path):\n",
    "    if frame is None:\n",
    "        if path is not None:\n",
    "            frame = import_a_set(path)\n",
    "        else:\n",
    "            raise Exception(\"Error : Provide a dataframe to split\")\n",
    "    return frame\n",
    "    \n",
    "def import_a_set(path=\"data/Data_A.csv\",is_test=False):\n",
    "    frame = pd.read_csv(path,header=None)\n",
    "    if is_test:\n",
    "        frame.columns = [c for c in COLUMNS if c!=\"LABEL\"]\n",
    "    else:\n",
    "        frame.columns = COLUMNS\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_a_set().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalization(serie):\n",
    "    return(serie-serie.mean())/serie.std()\n",
    "def min_max_scaling(serie):\n",
    "    return (serie - serie.min())/(serie.max()-serie.min())\n",
    "\n",
    "def measure_random(sample):\n",
    "    score =0\n",
    "    for index in range(len(sample)-1):\n",
    "        if sample[index]!=sample[index+1]:\n",
    "            score+=1\n",
    "    return score\n",
    "\n",
    "def measure_entropy(sample):\n",
    "    c = Counter(sample)\n",
    "    e = 0\n",
    "    for v in c.values():\n",
    "        e-= v/len(sample) * np.log(v/len(sample))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_label(frame):\n",
    "    frame[\"LABEL\"] = frame[\"LABEL\"] *2 -1\n",
    "    frame[\"LABEL_UP\"] = frame[\"LABEL\"].apply(lambda x : 1 if x == 1 else 0)\n",
    "    frame[\"LABEL_DOWN\"] = frame[\"LABEL\"].apply(lambda x : 1 if x == -1 else 0)\n",
    "    frame =frame.drop(\"LABEL\",axis=1)\n",
    "    return frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_features_6(frame=None,path=\"data/Data_A.csv\",no_label=False):\n",
    "    \"\"\"\n",
    "    Frame : pd.DF or None => (Will fetch A set if None)\n",
    "    path : str or None => Will fetch frame @ path if no frame is provided\n",
    "    \n",
    "    Rescales/adds feature/ applies non-linear tranformation to the features (No LABEL)\n",
    "    \"\"\"\n",
    "    frame = _norm_input_function(frame,path)\n",
    "    \n",
    "    frame[\"MID_P\"] = (frame[\"A_P_L1\"] + frame[\"B_P_L1\"]) / 2\n",
    "\n",
    "    for side in ['A','B']:\n",
    "        for level in range(1,5):\n",
    "            mean = frame[f\"{side}_P_L{level}\"].mean()\n",
    "            frame[f\"{side}_P_L{level}_UM\"] = frame[f\"{side}_P_L{level}\"].apply(lambda x : max(mean-x,0))\n",
    "            frame[f\"{side}_P_L{level}_OM\"] = frame[f\"{side}_P_L{level}\"].apply(lambda x : max(x-mean,0))\n",
    "\n",
    "    #GAP between levels of same side\n",
    "    frame[\"A_D_1\"] = (frame[\"A_P_L1\"] - frame[\"MID_P\"])\n",
    "    frame[\"A_D_2\"] = (frame[\"A_P_L2\"] - frame[\"A_P_L1\"])\n",
    "    frame[\"A_D_3\"] = (frame[\"A_P_L3\"] - frame[\"A_P_L2\"])\n",
    "    frame[\"A_D_4\"] = (frame[\"A_P_L4\"] - frame[\"A_P_L3\"])\n",
    "\n",
    "    frame[\"B_D_1\"] = (frame[\"B_P_L1\"] - frame[\"MID_P\"])\n",
    "    frame[\"B_D_2\"] = (frame[\"B_P_L2\"] - frame[\"B_P_L1\"])\n",
    "    frame[\"B_D_3\"] = (frame[\"B_P_L3\"] - frame[\"B_P_L2\"])\n",
    "    frame[\"B_D_4\"] = (frame[\"B_P_L4\"] - frame[\"B_P_L3\"])\n",
    "\n",
    "    frame[\"LC_FP\"] = (frame[\"LC_1\"]  + frame[\"LC_2\"]) - (frame[\"LC_3\"]  + frame[\"LC_4\"] + frame[\"LC_5\"])\n",
    "    frame[\"LC_FP_R\"] = (frame[\"LC_1\"]  + frame[\"LC_2\"]) / (frame[\"LC_3\"]  + frame[\"LC_4\"] + frame[\"LC_5\"]).apply(lambda x : max(1,x))\n",
    "\n",
    "    frame[\"LC_CONCAT_RANDOM\"] = (frame[\"LC_1\"].apply(str) + frame[\"LC_2\"].apply(str) +  frame[\"LC_3\"].apply(str)  + frame[\"LC_4\"].apply(str) + frame[\"LC_5\"].apply(str)).apply(measure_random)\n",
    "    frame[\"LC_CONCAT_ENTROPY\"] = (frame[\"LC_1\"].apply(str) + frame[\"LC_2\"].apply(str) +  frame[\"LC_3\"].apply(str)  + frame[\"LC_4\"].apply(str) + frame[\"LC_5\"].apply(str)).apply(measure_entropy)\n",
    "\n",
    "    frame[\"SUM_V_A\"] = frame[\"A_V_L1\"] + frame[\"A_V_L2\"] + frame[\"A_V_L3\"] + frame[\"A_V_L4\"]\n",
    "    frame[\"SUM_V_B\"] = frame[\"B_V_L1\"] + frame[\"B_V_L2\"] + frame[\"B_V_L3\"] + frame[\"B_V_L4\"]\n",
    "\n",
    "    frame[\"V_M_L\"] = frame[\"SUM_V_A\"] - frame[\"SUM_V_B\"]\n",
    "    frame[\"V_M_L\"] = frame[\"V_M_L\"].apply(lambda x: 1 if x>0 else -1)\n",
    "\n",
    "    frame[\"WPV_MID_DIF\"] = -frame[\"MID_P\"] * (frame[\"SUM_V_A\"]+frame[\"SUM_V_B\"])\n",
    "    for side in [\"A\",\"B\"]:\n",
    "        for level in [\"L1\",\"L2\",\"L3\",\"L4\"]:\n",
    "            frame[f\"WPV_{side}_{level}\"] = frame[f\"{side}_P_{level}\"] * frame[f\"{side}_V_{level}\"]\n",
    "            frame[\"WPV_MID_DIF\"] += frame[f\"WPV_{side}_{level}\"]\n",
    "            frame[f\"{side}_V_{level}_PCT\"] = frame[f\"{side}_V_{level}\"] / (frame[\"SUM_V_A\"]+frame[\"SUM_V_B\"]) \n",
    "            frame[f\"WPV_{side}_{level}_PCT\"] = frame[f\"{side}_P_{level}\"] * frame[f\"{side}_V_{level}_PCT\"]\n",
    "\n",
    "    for side in ['A','B']:\n",
    "        for level in range(1,5):\n",
    "            frame[f\"{side}_V_L{level}_LOG\"] = np.log(frame[f\"{side}_V_L{level}\"])\n",
    "    for col in frame.columns:\n",
    "        if \"LABEL\" not in col and \"LC\" not in col and col!=\"V_M_L\":\n",
    "            frame[col] = (mean_normalization(frame[col]))\n",
    "    for col in  [\"LC_1\",\"LC_2\",\"LC_3\",\"LC_4\",\"LC_5\"]:\n",
    "        frame[col] = frame[col]*2 -1\n",
    "    frame[\"LC_FP\"] = mean_normalization(frame[\"LC_FP\"])\n",
    "    frame[\"LC_FP_R\"] = mean_normalization(frame[\"LC_FP_R\"])\n",
    "    frame[\"LC_CONCAT_RANDOM\"] = mean_normalization(frame[\"LC_CONCAT_RANDOM\"])\n",
    "    frame[\"LC_CONCAT_ENTROPY\"] = mean_normalization(frame[\"LC_CONCAT_ENTROPY\"])\n",
    "\n",
    "    if not no_label:\n",
    "        frame = refactor_label(frame)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_features_0(frame=None,path=\"data/Data_A.csv\",no_label=False): #No scaling, no feature added\n",
    "    frame = _norm_input_function(frame,path)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_features_1(frame=None,path=\"data/Data_A.csv\",no_label=False): # stanardization scaling of the base feature\n",
    "    frame = _norm_input_function(frame,path)\n",
    "\n",
    "    for col in frame.columns:\n",
    "        if \"LC\" not in col and \"LABEL\" not in col:\n",
    "            frame[col] = mean_normalization(frame[col])\n",
    "    if not no_label:\n",
    "        frame = refactor_label(frame)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_features_6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_features_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(frame = None,test_ratio = 0.1,path=\"Data/Data_A.csv\"):\n",
    "    \n",
    "    frame = _norm_input_function(frame,path)\n",
    "    test_frame = frame.sample(frac=test_ratio).reset_index(drop=True)\n",
    "    train_frame = frame.drop(test_frame.index).reset_index(drop=True)\n",
    "\n",
    "    return train_frame,test_frame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(path=\"data/Data_A.csv\",feat_function:int=6,test_ratio:float = 0.1,split=True,split_before_scaling=True,no_label=False):\n",
    "    frame = _norm_input_function(None,path)\n",
    "\n",
    "    feature_engineering_fn  = globals()[f'rescale_features_{feat_function}']\n",
    "    if split:\n",
    "        if split_before_scaling:\n",
    "            train_frame,test_frame = split_training_data(frame)\n",
    "\n",
    "            train_frame = feature_engineering_fn(train_frame,no_label)\n",
    "            test_frame = feature_engineering_fn(test_frame,no_label)\n",
    "\n",
    "            return [train_frame,test_frame],train_frame.shape\n",
    "        else:\n",
    "            frame = feature_engineering_fn(frame,no_label)\n",
    "            train_frame,test_frame = split_training_data(frame)\n",
    "            return [train_frame,test_frame],train_frame.shape\n",
    "    else:\n",
    "        frame = feature_engineering_fn(frame,no_label)\n",
    "        return [frame],frame.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data viz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last tick change viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_recent_tick():\n",
    "    [df],_ = main_pipeline(feat_function=0,split=False)\n",
    "\n",
    "    df = df.drop([c for c in df.columns if (\"LC\" not in c and c!=\"LABEL\")],axis=1)\n",
    "    df[\"CONCAT\"] = df[\"LC_1\"].apply(str) + df[\"LC_2\"].apply(str) +  df[\"LC_3\"].apply(str)  + df[\"LC_4\"].apply(str) + df[\"LC_5\"].apply(str)\n",
    "    df = df.drop([c for c in df.columns if \"LC\" in c],axis = 1)\n",
    "    bins = df[\"LABEL\"].unique()\n",
    "    df_0 = df[df[\"LABEL\"]==0].drop(\"LABEL\",axis=1).value_counts(sort=True)\n",
    "    df_1 = df[df[\"LABEL\"]==1].drop(\"LABEL\",axis=1).value_counts(sort=True)\n",
    "\n",
    "    df_count = pd.DataFrame(columns=[\"0\",\"1\"])\n",
    "    df_count[\"0\"] = df_0\n",
    "    df_count[\"1\"] = df_1\n",
    "    df_count['ratio'] = (df_count[\"0\"]/df_count[\"1\"]).apply(lambda x : max(x,1/x))\n",
    "    df_count = df_count.sort_values(\"ratio\",ascending=False).drop(\"ratio\",axis=1)\n",
    "\n",
    "    df_count.index = [f\"{k[0]}\" for k in list(df_count.index)]\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot([k for k in df_count.index],[round(measure_entropy(k),2) for k in df_count.index],color=\"red\",linestyle=\"dashed\",label=\"Entropy\")\n",
    "    ax.plot([k for k in df_count.index],[round(measure_random(k),2) for k in df_count.index],color=\"blue\",linestyle=\"dashed\",label=\"Random\")\n",
    "\n",
    "    bar_plot =df_count.plot(ax=ax,kind='bar', rot=0, xlabel='Serie', ylabel='Value', title='Distribution of Labels',secondary_y=True)\n",
    "    # add some labels\n",
    "    for c in ax.containers:\n",
    "        # set the bar label\n",
    "        ax.bar_label(c, fmt='%.0f', label_type='edge')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "    # move the legend out of the plot\n",
    "    #ax.legend(handles= [bar_plot],title='Columns', bbox_to_anchor=(1, 1.02), loc='upper left')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_tick()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_fig(frame,col,col_label,suffix=\"\",bins=50,function = lambda x : x,title_suffix=\"\"):\n",
    "    side = [\"A\",\"B\"]\n",
    "    color=[\"blue\",\"red\"]\n",
    "    side_label=[\"ASK\",\"BID\"]\n",
    "    fig1, axs = plt.subplots(2,5,figsize=(40,10))\n",
    "    fig2,ax2 = plt.subplots(1,1,figsize=(5,5))\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            ax = axs[i,j]\n",
    "            ax.set_title(f\"{side_label[i]} {col_label} - Level : {j+1}\")\n",
    "            ax.set_xlabel(col_label)\n",
    "            plt.xticks(rotation = 45)\n",
    "            frame[f\"{side[i]}_{col}_L{j+1}{suffix}\"].apply(function).plot(kind=\"hist\",bins=bins,color=color[i],ax=ax)\n",
    "            frame[f\"{side[i]}_{col}_L{j+1}{suffix}\"].apply(function).plot(kind=\"hist\",bins=bins,color=color[i],alpha=0.2,ax=axs[i,4])\n",
    "            frame[f\"{side[i]}_{col}_L{j+1}{suffix}\"].apply(function).plot(kind=\"hist\",bins=bins,color=color[i],alpha=0.2,ax=ax2)\n",
    "    axs[0,4].set_title(f\"ASK {col_label} - all levels\")\n",
    "    axs[1,4].set_title(f\"BID {col_label} - all levels\")\n",
    "    ax2.set_title(f\"All {title_suffix} {col_label} aggregated\")\n",
    "    return fig1,fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[frame],_ = main_pipeline(feat_function=6,split=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_  = level_fig(frame,\"V\",\"Volumes\",bins=500) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = level_fig(frame,\"V\",\"Volumes\",bins=500,function=np.log,title_suffix=\"log\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = level_fig(frame,\"P\",\"Prices\",suffix=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prices Over/under mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = level_fig(frame,\"P\",\"Prices\",suffix=\"_OM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ = level_fig(frame,\"P\",\"Prices\",suffix=\"_UM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json file are pre-generated (output of a gridsearch function which is very time-consumming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_results(paths = [\"cv_result_x_cloud.json\",\"cv_result_x.json\"],view_angle = 260,elev = 20):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    axs = []\n",
    "    for i,path in enumerate(paths):\n",
    "        df = pd.read_json(path)\n",
    "        ax = fig.add_subplot(1, len(paths), i+1, projection='3d')\n",
    "        axs.append(ax)\n",
    "        colors = plt.cm.Spectral(df[\"mean_acc\"].values/df[\"mean_acc\"].values.max())\n",
    "        batch_size = sorted(list(df[\"batch_size\"].unique()))\n",
    "        batch_dict = {b:2*i for i,b in enumerate(batch_size)}\n",
    "        epochs = sorted(list(df[\"epochs\"].unique()))\n",
    "        epoch_dict = {e:2*i for i,e in enumerate(epochs)}\n",
    "        x = []\n",
    "        y= []\n",
    "        z = []\n",
    "        dx = []\n",
    "        dy = []\n",
    "        dz = []\n",
    "        for _,row in df.iterrows():\n",
    "            x.append(batch_dict.get(row[\"batch_size\"]))\n",
    "            y.append(epoch_dict.get(row[\"epochs\"]))\n",
    "            z.append(0)\n",
    "            dx.append(1)\n",
    "            dy.append(1)\n",
    "            dz.append(row[\"mean_acc\"])\n",
    "\n",
    "        bb = ax.bar3d(x, y, z, dx, dy, dz, color=colors,cmap = plt.cm.Spectral)\n",
    "        ax.set_xticks(list(batch_dict.values()))\n",
    "        ax.set_yticks(list(epoch_dict.values()))\n",
    "        ax.set_xticklabels(batch_size)\n",
    "        ax.set_yticklabels(epochs)\n",
    "        ax.set_title(\"Grid search of the optimal batch-size/epochs combination\")\n",
    "        ax.set_xlabel('Batch size')\n",
    "        ax.set_ylabel('Epochs')\n",
    "        ax.set_zlabel('Mean accuracy')\n",
    "        ax.view_init(elev=elev, azim=view_angle)\n",
    "\n",
    "    fig.colorbar(bb,label=\"Accuracy\", orientation=\"horizontal\",shrink=True, aspect=10,ax=axs)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_results(elev=22,view_angle=260)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_proba(row):\n",
    "    if row[0]>row[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def custom_accuracy(prediction : np.array,y_test):\n",
    "    y_hat_values = np.apply_along_axis(class_proba, 1, prediction)\n",
    "    y_test_values = np.apply_along_axis(class_proba,1,y_test.values)\n",
    "    res = np.equal(y_hat_values,y_test_values)\n",
    "    count = np.bincount(res)\n",
    "    return count,len(res),count/len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(plot=False,input_shape=86,hidden_neurons=100):\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_shape,)),\n",
    "        keras.layers.Dense(hidden_neurons, activation=\"tanh\",name=\"Hidden_layer_1\"),\n",
    "        keras.layers.Dense(hidden_neurons, activation=\"tanh\",name=\"Hidden_layer_2\"),\n",
    "        keras.layers.Dense(hidden_neurons, activation=\"tanh\",name=\"Hidden_layer_3\"),\n",
    "        keras.layers.Dense(2, activation=\"softmax\",name=\"Output\")\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    if plot:\n",
    "        plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(history):\n",
    "    h_values = history.history['accuracy']\n",
    "    h_delta = [np.log(h_values[i+1]/h_values[i]) for i in range(len(h_values)-1)]\n",
    "    fig,axs = plt.subplots(2,1,figsize=(15,8),sharex = True)\n",
    "    ax1 = axs[0]\n",
    "    ax1.plot(h_values)\n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.legend(['train accuracy'], loc='upper left')\n",
    "    ax2 = axs[1]\n",
    "    ax2.plot(h_delta,color=\"orange\")\n",
    "    ax2.plot([0 for _ in range(len(h_delta))], color = \"b\",linestyle=\"dashed\")\n",
    "    ax2.set_title('Accuracy Delta')\n",
    "    ax2.set_ylabel('$\\Delta_{Accuracy}$')\n",
    "    ax2.legend(['train delta'], loc='upper left')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    #fig.savefig(f\"output/model_train_b{batch_size}_e{epoch}.png\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_from_pipeline(model = None,feat_function=6,epoch = 600,batch_size = 1000,plot_accuracy = False,test_ratio = 0.1,prt=True,hidden_neurons=100,split_before_scaling = True):\n",
    "    \n",
    "    [train_frame,test_frame],input_shape = main_pipeline(feat_function=feat_function,test_ratio=test_ratio,split_before_scaling=split_before_scaling)\n",
    "   \n",
    "   \n",
    "    y_train = train_frame[[\"LABEL_DOWN\",\"LABEL_UP\"]]\n",
    "    x_train = train_frame.drop([\"LABEL_DOWN\",\"LABEL_UP\"],axis=1)\n",
    "    y_test = test_frame[[\"LABEL_DOWN\",\"LABEL_UP\"]]\n",
    "    x_test = test_frame.drop([\"LABEL_DOWN\",\"LABEL_UP\"],axis=1) \n",
    "\n",
    "    hidden_neurons = 30 if feat_function == 1 else hidden_neurons if hidden_neurons else 100\n",
    "    model = model if model else create_model(input_shape=input_shape[1]-2,hidden_neurons = hidden_neurons)\n",
    "\n",
    "    history = model.fit(x_train.to_numpy(), y_train.to_numpy(),epochs= epoch ,batch_size =batch_size,verbose=0)\n",
    "\n",
    "    if plot_accuracy:\n",
    "        fig = plot_acc(history)\n",
    "        fig.show()\n",
    "    \n",
    "    _, accuracy = model.evaluate(x_test.to_numpy(), y_test.to_numpy())\n",
    "    prediction = model.predict(x_test.to_numpy())\n",
    "    _,_,(_,a) = custom_accuracy(prediction,y_test)\n",
    "\n",
    "    if prt:\n",
    "        print(f\"Accuracy : {round(accuracy,4)*100}% || {a}\")\n",
    "\n",
    "    return accuracy  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some runs to asses the quality of the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering + scaling the test and train together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_from_pipeline(feat_function=6,plot_accuracy=True,prt=True,split_before_scaling=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering scaling the test and the train apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_from_pipeline(feat_function=6,plot_accuracy=True,prt=True,split_before_scaling=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of the base feature + scaling the test and train together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_from_pipeline(feat_function=1,plot_accuracy=True,prt=True,split_before_scaling=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of the base feature + scaling the test and train apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_from_pipeline(feat_function=1,plot_accuracy=True,prt=True,split_before_scaling=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of the labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing both dataset together and concatening them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_train = import_a_set(path=\"data/Data_A.csv\",is_test=False)\n",
    "feat_test = import_a_set(path=\"data/Data_B_nolabels.csv\",is_test=True)\n",
    "\n",
    "frame_train.index = [f\"train_{i}\" for i in frame_train.index]\n",
    "feat_test.index = [f\"test{i}\" for i in feat_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label_train = frame_train.drop(\"LABEL\",axis=1)\n",
    "scaled_feat = rescale_features_6(frame=pd.concat([no_label_train,feat_test]),no_label=True)\n",
    "\n",
    "x_train = scaled_feat[scaled_feat.index.isin(frame_train.index)]\n",
    "y_train = refactor_label(frame_train)[[\"LABEL_DOWN\",\"LABEL_UP\"]]\n",
    "x_test = scaled_feat[scaled_feat.index.isin(feat_test.index)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=x_train.shape[1])\n",
    "history = model.fit(x_train.to_numpy(),y_train.to_numpy(),batch_size=1000,epochs=650,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_acc(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_array = model.predict(x_test.to_numpy())\n",
    "y_test = pd.DataFrame(columns=[\"LABEL\"])\n",
    "y_test[\"LABEL\"] = np.apply_along_axis(class_proba,1,prediction_array)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data/02289032_DEDIEU.txt\", y_test.values, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83963c9b1196a370d61bf499154237051a7c6893ca3b77e3fd98a8801536fda0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
